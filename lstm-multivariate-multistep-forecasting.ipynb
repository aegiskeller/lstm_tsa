{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T01:34:29.917653Z",
     "iopub.status.busy": "2022-01-21T01:34:29.917425Z",
     "iopub.status.idle": "2022-01-21T01:34:29.989317Z",
     "shell.execute_reply": "2022-01-21T01:34:29.988591Z",
     "shell.execute_reply.started": "2022-01-21T01:34:29.91763Z"
    }
   },
   "outputs": [],
   "source": [
    "#lstm Multivariate MultiStep Example OVer Air Pollution Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from pandas import DataFrame , concat\n",
    "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from numpy import mean , concatenate\n",
    "from math import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,LSTM,Activation, Input\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "from numpy import array , hstack\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "#Before do anything else do not forget to reset the backend for the next iteration (rerun the model)\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"input/lstm-datasets-multivariate-univariate/LSTM-Multivariate_pollution.csv\", header=0, index_col=0)\n",
    "\n",
    "t = dataset.columns.tolist()\n",
    "\n",
    "dataset = dataset[['dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain','pollution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T01:34:29.991124Z",
     "iopub.status.busy": "2022-01-21T01:34:29.990892Z",
     "iopub.status.idle": "2022-01-21T01:34:30.017452Z",
     "shell.execute_reply": "2022-01-21T01:34:30.016377Z",
     "shell.execute_reply.started": "2022-01-21T01:34:29.991092Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#else slice is invalid for use in labelEncoder\n",
    "dataset= dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "dataset[:,3] = encoder.fit_transform(dataset[:,3])\n",
    "\n",
    "#conver to pd.Dataframe else slices error\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.columns = ['dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain','pollution']\n",
    "\n",
    "######## ensure all data is float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T01:34:30.019647Z",
     "iopub.status.busy": "2022-01-21T01:34:30.019255Z",
     "iopub.status.idle": "2022-01-21T01:34:30.05285Z",
     "shell.execute_reply": "2022-01-21T01:34:30.051653Z",
     "shell.execute_reply.started": "2022-01-21T01:34:30.019612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1.shape (43800, 1)\n",
      "x_2.shape (43800, 1)\n",
      "y.shape (43800, 1)\n",
      "dataset_stacked.shape (43800, 8)\n"
     ]
    }
   ],
   "source": [
    "#dataset[['dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain','pollution']]\n",
    "#Data Pre-processing step--------------------------------\n",
    "x_1 = dataset['dew'].values\n",
    "x_2 = dataset['temp'].values\n",
    "x_3 = dataset['press'].values\n",
    "x_4 = dataset['wnd_spd'].values\n",
    "x_5 = dataset['wnd_dir'].values\n",
    "x_6 = dataset['snow'].values\n",
    "x_7 = dataset['rain'].values\n",
    "y = dataset['pollution'].values\n",
    "#x_1 = x_1.values\n",
    "#x_2 = x_2.values\n",
    "#y = y.values\n",
    "\n",
    "\n",
    "# Step 1 : convert to [rows, columns] structure\n",
    "x_1 = x_1.reshape((len(x_1), 1))\n",
    "x_2 = x_2.reshape((len(x_2), 1))\n",
    "x_3 = x_3.reshape((len(x_3), 1))\n",
    "x_4 = x_4.reshape((len(x_4), 1))\n",
    "x_5 = x_5.reshape((len(x_5), 1))\n",
    "x_6 = x_6.reshape((len(x_6), 1))\n",
    "x_7 = x_7.reshape((len(x_7), 1))\n",
    "y = y.reshape((len(y), 1))\n",
    "print (\"x_1.shape\" , x_1.shape) \n",
    "print (\"x_2.shape\" , x_2.shape) \n",
    "print (\"y.shape\" , y.shape)\n",
    "\n",
    "# Step 2 : normalization \n",
    "scaler_x_1 = MinMaxScaler(feature_range=(0, 1)).fit(x_1)\n",
    "x_1_scaled = scaler_x_1.transform(x_1)\n",
    "scaler_x_2 = MinMaxScaler(feature_range=(0, 1)).fit(x_2)\n",
    "x_2_scaled = scaler_x_2.transform(x_2)\n",
    "scaler_x_3 = MinMaxScaler(feature_range=(0, 1)).fit(x_3)\n",
    "x_3_scaled = scaler_x_3.transform(x_3)\n",
    "scaler_x_4 = MinMaxScaler(feature_range=(0, 1)).fit(x_4)\n",
    "x_4_scaled = scaler_x_4.transform(x_4)\n",
    "scaler_x_5 = MinMaxScaler(feature_range=(0, 1)).fit(x_5)\n",
    "x_5_scaled = scaler_x_5.transform(x_5)\n",
    "scaler_x_6 = MinMaxScaler(feature_range=(0, 1)).fit(x_6)\n",
    "x_6_scaled = scaler_x_6.transform(x_6)\n",
    "scaler_x_7 = MinMaxScaler(feature_range=(0, 1)).fit(x_7)\n",
    "x_7_scaled = scaler_x_7.transform(x_7)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1)).fit(y)\n",
    "y_scaled = scaler_y.transform(y)\n",
    "\n",
    "# Step 3 : horizontally stack columns\n",
    "dataset_stacked = hstack((x_1_scaled, x_2_scaled,x_2_scaled, x_3_scaled,\n",
    "                          x_4_scaled, x_5_scaled,x_7_scaled, y_scaled))\n",
    "print (\"dataset_stacked.shape\" , dataset_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T01:34:30.054642Z",
     "iopub.status.busy": "2022-01-21T01:34:30.053932Z",
     "iopub.status.idle": "2022-01-21T01:34:30.167977Z",
     "shell.execute_reply": "2022-01-21T01:34:30.167282Z",
     "shell.execute_reply.started": "2022-01-21T01:34:30.054603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (43741, 60, 7)\n",
      "y.shape (43741, 1)\n"
     ]
    }
   ],
   "source": [
    "#1. n_steps_in : Specify how much data we want to look back for prediction\n",
    "#2. n_step_out : Specify how much multi-step data we want to forecast\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    " X, y = list(), list()\n",
    " for i in range(len(sequences)):\n",
    "  # find the end of this pattern\n",
    "  end_ix = i + n_steps_in\n",
    "  out_end_ix = end_ix + n_steps_out-1\n",
    "  # check if we are beyond the dataset\n",
    "  if out_end_ix > len(sequences):\n",
    "   break\n",
    "  # gather input and output parts of the pattern\n",
    "  seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "  X.append(seq_x)\n",
    "  y.append(seq_y)\n",
    " return array(X), array(y)\n",
    "\n",
    "# choose a number of time steps #change this accordingly\n",
    "n_steps_in, n_steps_out = 60 , 1#30\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset_stacked, n_steps_in, n_steps_out)\n",
    "print (\"X.shape\" , X.shape) \n",
    "print (\"y.shape\" , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T01:34:30.169364Z",
     "iopub.status.busy": "2022-01-21T01:34:30.169188Z",
     "iopub.status.idle": "2022-01-21T01:36:22.925861Z",
     "shell.execute_reply": "2022-01-21T01:36:22.924112Z",
     "shell.execute_reply.started": "2022-01-21T01:34:30.169343Z"
    }
   },
   "outputs": [],
   "source": [
    "#spliting the dataset--------------------------\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X,train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "train_X.shape\n",
    "train_y.shape \n",
    "test_X.shape \n",
    "test_y.shape \n",
    "n_features = 7\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34992, 60, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(64, 60, 7)]             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, 60, 10)              720       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, 60, 10)              840       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, 60, 1)               11        \n",
      "=================================================================\n",
      "Total params: 1,571\n",
      "Trainable params: 1,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#optimizer learning rate\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "# define model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(50, activation='relu', stateful=True, return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "# model.add(LSTM(50, activation='relu', stateful=True, return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "# model.add(Dense(n_steps_out))\n",
    "# model.add(Activation('linear'))\n",
    "# model.compile(loss='mse' , optimizer=opt , metrics=['accuracy'])\n",
    "\n",
    "input_1 = Input(batch_shape=(batch_size, n_steps_in, n_features))\n",
    "lstm_layer_1 = LSTM(10, stateful=True, return_sequences=True)(input_1)\n",
    "lstm_layer_2 = LSTM(10, stateful=True, return_sequences=True)(lstm_layer_1)\n",
    "output_1 = Dense(units=1)(lstm_layer_2)\n",
    "\n",
    "wildwilly=Model(inputs=input_1, outputs=output_1)\n",
    "wildwilly.summary()\n",
    "wildwilly.compile(loss='mse' , optimizer=opt , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.0469"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "   Specified a list with shape [64,7] from a tensor with shape [48,7]\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_4465]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2388/2913713780.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit network #increase the epochs for better model training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# history = model.fit(train_X , train_y , epochs=50, steps_per_epoch=25 , verbose=1 ,validation_data=(test_X, test_y) ,shuffle=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwildwilly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:    Specified a list with shape [64,7] from a tensor with shape [48,7]\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_4465]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit network #increase the epochs for better model training\n",
    "# history = model.fit(train_X , train_y , epochs=50, steps_per_epoch=25 , verbose=1 ,validation_data=(test_X, test_y) ,shuffle=False)\n",
    "history = wildwilly.fit(train_X , train_y , epochs=50, batch_size=batch_size , verbose=1 ,validation_data=(test_X, test_y) ,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-21T01:36:22.92702Z",
     "iopub.status.idle": "2022-01-21T01:36:22.92745Z",
     "shell.execute_reply": "2022-01-21T01:36:22.927272Z",
     "shell.execute_reply.started": "2022-01-21T01:36:22.927252Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TEST DATA----------------------------------------\n",
    "\n",
    "dataset_test_ok = pd.read_csv('input/lstm-datasets-multivariate-univariate/pollution_test_data1.csv')\n",
    "dataset_test_ok.head()\n",
    "\n",
    "# integer encode direction\n",
    "encoder1 = LabelEncoder()\n",
    "dataset_test_ok.iloc[:,3] = encoder1.fit_transform(dataset_test_ok.iloc[:,3])\n",
    "\n",
    "\n",
    "# read test data\n",
    "x1_test = dataset_test_ok['dew'].values\n",
    "x2_test = dataset_test_ok['temp'].values\n",
    "x3_test = dataset_test_ok['press'].values\n",
    "x4_test = dataset_test_ok['wnd_spd'].values\n",
    "x5_test = dataset_test_ok['wnd_dir'].values\n",
    "x6_test = dataset_test_ok['snow'].values\n",
    "x7_test = dataset_test_ok['rain'].values\n",
    "y_test = dataset_test_ok['pollution'].values # no need to scale\n",
    "\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "x1_test = x1_test.reshape((len(x1_test), 1))\n",
    "x2_test = x2_test.reshape((len(x2_test), 1))\n",
    "x3_test = x3_test.reshape((len(x3_test), 1))\n",
    "x4_test = x4_test.reshape((len(x4_test), 1))\n",
    "x5_test = x5_test.reshape((len(x5_test), 1))\n",
    "x6_test = x6_test.reshape((len(x6_test), 1))\n",
    "x7_test = x7_test.reshape((len(x7_test), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))\n",
    "\n",
    "\n",
    "x1_test_scaled = scaler_x_1.transform(x1_test)\n",
    "x2_test_scaled = scaler_x_2.transform(x2_test)\n",
    "x3_test_scaled = scaler_x_3.transform(x3_test)\n",
    "x4_test_scaled = scaler_x_4.transform(x4_test)\n",
    "x5_test_scaled = scaler_x_5.transform(x5_test)\n",
    "x6_test_scaled = scaler_x_6.transform(x6_test)\n",
    "x7_test_scaled = scaler_x_7.transform(x7_test)\n",
    "\n",
    "\n",
    "# Step 3 : horizontally stack columns\n",
    "dataset_test_stacked = hstack((x1_test_scaled,x2_test_scaled,x3_test_scaled,x4_test_scaled,\n",
    "                               x5_test_scaled,x6_test_scaled,x7_test_scaled))\n",
    "\n",
    "\n",
    "print (\"dataset_stacked.shape\" , dataset_test_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-21T01:36:22.928562Z",
     "iopub.status.idle": "2022-01-21T01:36:22.928843Z",
     "shell.execute_reply": "2022-01-21T01:36:22.928718Z",
     "shell.execute_reply.started": "2022-01-21T01:36:22.928704Z"
    }
   },
   "outputs": [],
   "source": [
    "###Prediction#######################################################\n",
    "\n",
    "dataset_test_X = dataset_test_stacked\n",
    "print(\"dataset_test_X :\",dataset_test_X.shape)\n",
    "test_X_new = dataset_test_X.reshape(1,dataset_test_X.shape[0],dataset_test_X.shape[1])\n",
    "\n",
    "y_pred = model.predict(test_X_new)\n",
    "\n",
    "y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
    "y_pred_inv = y_pred_inv.reshape(n_steps_out,1)\n",
    "y_pred_inv = y_pred_inv[:,0]\n",
    "print(\"y_pred :\",y_pred.shape)\n",
    "print(\"y_pred_inv :\",y_pred_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test)\n",
    "plt.plot(y_pred_inv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
